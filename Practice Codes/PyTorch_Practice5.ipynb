{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_Practice5.ipynb","provenance":[],"authorship_tag":"ABX9TyPuVirEmYMnffOjMLK1Ndwl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"WXKqklcB7kju","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import random\n","import tensorflow as tf\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj-AFqgc9VLF","colab_type":"code","outputId":"b67abc71-8be4-4e83-aab5-b0b1066e8d89","executionInfo":{"status":"ok","timestamp":1586455855961,"user_tz":-330,"elapsed":5744,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#implementing 2 layer network using numpy only\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x=np.random.randn(N, D_in)\n","y=np.random.randn(N, D_out)\n","w1=np.random.randn(D_in, H)\n","w2=np.random.randn(H, D_out)\n","lr=1e-6\n","\n","iter_no=list()\n","cost_for_iter=list()\n","for iteration in range(500):\n","\n","    h=x.dot(w1)\n","    h_relu=np.maximum(h, 0)\n","    y_pred=h_relu.dot(w2)\n","\n","    cost=np.square(y_pred-y).sum()\n","    print('iteration no: {0}, loss: {1}'.format(iteration, cost))\n","\n","    iter_no.append(iteration)\n","    cost_for_iter.append(cost)\n","    #gradient calculation\n","\n","    grad_y_pred=2*(y_pred-y)            #shape is N, D_out\n","    grad_w2=h_relu.T.dot(grad_y_pred)    #shape is 100, 10\n","    grad_h_relu=grad_y_pred.dot(w2.T)   #shape is N, 100\n","    grad_h=grad_h_relu.copy()    #shape is N, 100\n","    grad_h[h<0]=0       #shape is N, 100\n","    grad_w1=x.T.dot(grad_h)         #shape is 1000, 100\n","\n","    #updating the parameters\n","\n","    w1-=lr*grad_w1\n","    w2-=lr*grad_w2\n","\n","\n","plt.plot(iter_no, cost_for_iter)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iteration no: 0, loss: 29838926.444667853\n","iteration no: 1, loss: 25949868.98528431\n","iteration no: 2, loss: 26220269.06554956\n","iteration no: 3, loss: 26517632.130325228\n","iteration no: 4, loss: 24490641.516573627\n","iteration no: 5, loss: 19365951.723266084\n","iteration no: 6, loss: 13177617.740311315\n","iteration no: 7, loss: 7924915.577720184\n","iteration no: 8, loss: 4542644.2354776785\n","iteration no: 9, loss: 2642597.328520773\n","iteration no: 10, loss: 1648200.8976480241\n","iteration no: 11, loss: 1122292.1819825342\n","iteration no: 12, loss: 829470.6984083661\n","iteration no: 13, loss: 651911.9001532556\n","iteration no: 14, loss: 533881.3821030419\n","iteration no: 15, loss: 448671.31101604004\n","iteration no: 16, loss: 383204.0429058899\n","iteration no: 17, loss: 330685.69458516885\n","iteration no: 18, loss: 287368.2028390303\n","iteration no: 19, loss: 251086.6441492751\n","iteration no: 20, loss: 220336.98040523886\n","iteration no: 21, loss: 194044.7633997909\n","iteration no: 22, loss: 171461.45428411986\n","iteration no: 23, loss: 151940.31560253983\n","iteration no: 24, loss: 134994.12165873667\n","iteration no: 25, loss: 120224.55739058962\n","iteration no: 26, loss: 107317.29848595182\n","iteration no: 27, loss: 96014.61306255378\n","iteration no: 28, loss: 86093.48530343626\n","iteration no: 29, loss: 77353.28712919472\n","iteration no: 30, loss: 69617.71505482361\n","iteration no: 31, loss: 62779.62744496297\n","iteration no: 32, loss: 56712.14012035808\n","iteration no: 33, loss: 51310.67989694872\n","iteration no: 34, loss: 46489.36012380648\n","iteration no: 35, loss: 42178.288029541494\n","iteration no: 36, loss: 38316.09737533678\n","iteration no: 37, loss: 34849.887107578645\n","iteration no: 38, loss: 31737.291971585357\n","iteration no: 39, loss: 28936.819040811766\n","iteration no: 40, loss: 26412.32707170654\n","iteration no: 41, loss: 24136.121312319803\n","iteration no: 42, loss: 22080.692128690756\n","iteration no: 43, loss: 20220.69177919202\n","iteration no: 44, loss: 18536.151981517614\n","iteration no: 45, loss: 17008.602439661052\n","iteration no: 46, loss: 15620.410619022026\n","iteration no: 47, loss: 14357.810524659413\n","iteration no: 48, loss: 13208.81558056792\n","iteration no: 49, loss: 12162.675385256854\n","iteration no: 50, loss: 11208.118996722507\n","iteration no: 51, loss: 10337.65507671511\n","iteration no: 52, loss: 9542.500555941027\n","iteration no: 53, loss: 8814.36715364728\n","iteration no: 54, loss: 8147.2540152056745\n","iteration no: 55, loss: 7536.096604267674\n","iteration no: 56, loss: 6975.053527382184\n","iteration no: 57, loss: 6459.739631621042\n","iteration no: 58, loss: 5986.450867203819\n","iteration no: 59, loss: 5550.922908383871\n","iteration no: 60, loss: 5150.015849141547\n","iteration no: 61, loss: 4780.872808154429\n","iteration no: 62, loss: 4440.666729889503\n","iteration no: 63, loss: 4126.809772294171\n","iteration no: 64, loss: 3837.07902480873\n","iteration no: 65, loss: 3569.458261671815\n","iteration no: 66, loss: 3322.0550099468205\n","iteration no: 67, loss: 3093.400974720439\n","iteration no: 68, loss: 2881.994829417935\n","iteration no: 69, loss: 2686.2414856222235\n","iteration no: 70, loss: 2504.928024860889\n","iteration no: 71, loss: 2337.0185688224515\n","iteration no: 72, loss: 2181.076552596077\n","iteration no: 73, loss: 2036.3762279859166\n","iteration no: 74, loss: 1902.106041244295\n","iteration no: 75, loss: 1777.3262927147348\n","iteration no: 76, loss: 1661.3663025307837\n","iteration no: 77, loss: 1553.5614007502625\n","iteration no: 78, loss: 1453.2849517421764\n","iteration no: 79, loss: 1359.984101693542\n","iteration no: 80, loss: 1273.1486410217703\n","iteration no: 81, loss: 1192.2481091862646\n","iteration no: 82, loss: 1116.8486931227853\n","iteration no: 83, loss: 1046.5577508298588\n","iteration no: 84, loss: 981.0385675755275\n","iteration no: 85, loss: 919.8915896402077\n","iteration no: 86, loss: 862.8074772312259\n","iteration no: 87, loss: 809.5215187945647\n","iteration no: 88, loss: 759.7674288093089\n","iteration no: 89, loss: 713.2845801434871\n","iteration no: 90, loss: 669.8235298455196\n","iteration no: 91, loss: 629.2037469942208\n","iteration no: 92, loss: 591.235059336673\n","iteration no: 93, loss: 555.6863660341276\n","iteration no: 94, loss: 522.4182192317411\n","iteration no: 95, loss: 491.2705334325128\n","iteration no: 96, loss: 462.1022041136208\n","iteration no: 97, loss: 434.77370223085995\n","iteration no: 98, loss: 409.1606694882946\n","iteration no: 99, loss: 385.1482602523945\n","iteration no: 100, loss: 362.6596151449346\n","iteration no: 101, loss: 341.54093096139616\n","iteration no: 102, loss: 321.71607696968454\n","iteration no: 103, loss: 303.11627207308584\n","iteration no: 104, loss: 285.6575697081173\n","iteration no: 105, loss: 269.2652880929859\n","iteration no: 106, loss: 253.86080534248902\n","iteration no: 107, loss: 239.38602634151783\n","iteration no: 108, loss: 225.7918943387411\n","iteration no: 109, loss: 213.01466153252213\n","iteration no: 110, loss: 201.00605144997314\n","iteration no: 111, loss: 189.7097823830911\n","iteration no: 112, loss: 179.07462107099713\n","iteration no: 113, loss: 169.06929764437763\n","iteration no: 114, loss: 159.65546945034987\n","iteration no: 115, loss: 150.79017195936018\n","iteration no: 116, loss: 142.44550665961023\n","iteration no: 117, loss: 134.60261284746267\n","iteration no: 118, loss: 127.21290338776035\n","iteration no: 119, loss: 120.25097659292189\n","iteration no: 120, loss: 113.68754564730492\n","iteration no: 121, loss: 107.50635344552835\n","iteration no: 122, loss: 101.67633277166313\n","iteration no: 123, loss: 96.17548983182873\n","iteration no: 124, loss: 90.98655538989314\n","iteration no: 125, loss: 86.09208129892521\n","iteration no: 126, loss: 81.47176918976515\n","iteration no: 127, loss: 77.11124747633815\n","iteration no: 128, loss: 72.99450024380721\n","iteration no: 129, loss: 69.10832748879562\n","iteration no: 130, loss: 65.43905687332878\n","iteration no: 131, loss: 61.97419845459939\n","iteration no: 132, loss: 58.699231509900656\n","iteration no: 133, loss: 55.60994124618624\n","iteration no: 134, loss: 52.68566945252134\n","iteration no: 135, loss: 49.923333986569745\n","iteration no: 136, loss: 47.31072408550915\n","iteration no: 137, loss: 44.84134508037688\n","iteration no: 138, loss: 42.50582216337056\n","iteration no: 139, loss: 40.29656579162456\n","iteration no: 140, loss: 38.2068796366617\n","iteration no: 141, loss: 36.22997697654746\n","iteration no: 142, loss: 34.35957356944192\n","iteration no: 143, loss: 32.58907741923636\n","iteration no: 144, loss: 30.91462659248952\n","iteration no: 145, loss: 29.331340947962335\n","iteration no: 146, loss: 27.82998993887053\n","iteration no: 147, loss: 26.407786634866795\n","iteration no: 148, loss: 25.06090287855293\n","iteration no: 149, loss: 23.78570180637348\n","iteration no: 150, loss: 22.577348098674207\n","iteration no: 151, loss: 21.432642939735686\n","iteration no: 152, loss: 20.348673921524025\n","iteration no: 153, loss: 19.320728250363103\n","iteration no: 154, loss: 18.346374400783702\n","iteration no: 155, loss: 17.422878339161677\n","iteration no: 156, loss: 16.548417007471286\n","iteration no: 157, loss: 15.71938303360959\n","iteration no: 158, loss: 14.932239234513114\n","iteration no: 159, loss: 14.186712564701327\n","iteration no: 160, loss: 13.479070927934682\n","iteration no: 161, loss: 12.8076884763912\n","iteration no: 162, loss: 12.1707622298507\n","iteration no: 163, loss: 11.566366724030459\n","iteration no: 164, loss: 10.992815070575654\n","iteration no: 165, loss: 10.448695842452574\n","iteration no: 166, loss: 9.932205625781688\n","iteration no: 167, loss: 9.441966454026325\n","iteration no: 168, loss: 8.977084032076641\n","iteration no: 169, loss: 8.535941541772736\n","iteration no: 170, loss: 8.116489774985995\n","iteration no: 171, loss: 7.7181575434350265\n","iteration no: 172, loss: 7.340028173843683\n","iteration no: 173, loss: 6.980934869761157\n","iteration no: 174, loss: 6.6397326569218995\n","iteration no: 175, loss: 6.315680732275874\n","iteration no: 176, loss: 6.007789730883873\n","iteration no: 177, loss: 5.7154788014885565\n","iteration no: 178, loss: 5.437540356084366\n","iteration no: 179, loss: 5.1735072488986455\n","iteration no: 180, loss: 4.922629333292448\n","iteration no: 181, loss: 4.684582411636015\n","iteration no: 182, loss: 4.458054930216353\n","iteration no: 183, loss: 4.242732059747263\n","iteration no: 184, loss: 4.037911572471\n","iteration no: 185, loss: 3.843238975046095\n","iteration no: 186, loss: 3.658175609516698\n","iteration no: 187, loss: 3.482172698184436\n","iteration no: 188, loss: 3.314781407886714\n","iteration no: 189, loss: 3.1556644570101247\n","iteration no: 190, loss: 3.004332407517595\n","iteration no: 191, loss: 2.86037315134866\n","iteration no: 192, loss: 2.723454595804201\n","iteration no: 193, loss: 2.5934281183729953\n","iteration no: 194, loss: 2.4695944759041177\n","iteration no: 195, loss: 2.3517475003465673\n","iteration no: 196, loss: 2.23965786340624\n","iteration no: 197, loss: 2.1329416693871797\n","iteration no: 198, loss: 2.0314287873756527\n","iteration no: 199, loss: 1.9348740937563003\n","iteration no: 200, loss: 1.8429555453588673\n","iteration no: 201, loss: 1.7554655974179214\n","iteration no: 202, loss: 1.672236289567169\n","iteration no: 203, loss: 1.5929932827532078\n","iteration no: 204, loss: 1.5175513662837459\n","iteration no: 205, loss: 1.4458529255134973\n","iteration no: 206, loss: 1.3776735597659182\n","iteration no: 207, loss: 1.3125988690541366\n","iteration no: 208, loss: 1.2506531857920722\n","iteration no: 209, loss: 1.1916653636220804\n","iteration no: 210, loss: 1.135509542691882\n","iteration no: 211, loss: 1.0820464117947193\n","iteration no: 212, loss: 1.0311309922577858\n","iteration no: 213, loss: 0.9826300148299851\n","iteration no: 214, loss: 0.9364561245628511\n","iteration no: 215, loss: 0.8924868020454332\n","iteration no: 216, loss: 0.8506148070448858\n","iteration no: 217, loss: 0.8107322599242561\n","iteration no: 218, loss: 0.7728143763881923\n","iteration no: 219, loss: 0.7366177487175665\n","iteration no: 220, loss: 0.702143786967275\n","iteration no: 221, loss: 0.6693014374200952\n","iteration no: 222, loss: 0.6380134846169834\n","iteration no: 223, loss: 0.6082056768132114\n","iteration no: 224, loss: 0.5797996793055142\n","iteration no: 225, loss: 0.5527546266094454\n","iteration no: 226, loss: 0.5269828667112557\n","iteration no: 227, loss: 0.5024275713508755\n","iteration no: 228, loss: 0.4790265340223332\n","iteration no: 229, loss: 0.456747642685982\n","iteration no: 230, loss: 0.43552715583824675\n","iteration no: 231, loss: 0.4152947558680335\n","iteration no: 232, loss: 0.39598900879738563\n","iteration no: 233, loss: 0.3775924740379545\n","iteration no: 234, loss: 0.36006158721872583\n","iteration no: 235, loss: 0.3433506079516257\n","iteration no: 236, loss: 0.3274223414279326\n","iteration no: 237, loss: 0.3122375208317577\n","iteration no: 238, loss: 0.2977676247586611\n","iteration no: 239, loss: 0.2839850837139556\n","iteration no: 240, loss: 0.2708373513314737\n","iteration no: 241, loss: 0.25830047168362463\n","iteration no: 242, loss: 0.24635041030044724\n","iteration no: 243, loss: 0.2349827675907621\n","iteration no: 244, loss: 0.2241238818250001\n","iteration no: 245, loss: 0.21376862022818421\n","iteration no: 246, loss: 0.20389803312966612\n","iteration no: 247, loss: 0.19448529319234847\n","iteration no: 248, loss: 0.18551369255814426\n","iteration no: 249, loss: 0.17695858325058672\n","iteration no: 250, loss: 0.16880203937660218\n","iteration no: 251, loss: 0.161020963215769\n","iteration no: 252, loss: 0.15361131220388757\n","iteration no: 253, loss: 0.14654347219441066\n","iteration no: 254, loss: 0.13979659326008712\n","iteration no: 255, loss: 0.1333744109778257\n","iteration no: 256, loss: 0.1272426184187512\n","iteration no: 257, loss: 0.12139282896602106\n","iteration no: 258, loss: 0.11581429689731634\n","iteration no: 259, loss: 0.11049087821302042\n","iteration no: 260, loss: 0.10541629744558415\n","iteration no: 261, loss: 0.1005765314450031\n","iteration no: 262, loss: 0.09595999321506495\n","iteration no: 263, loss: 0.09155815899915348\n","iteration no: 264, loss: 0.08735761929215903\n","iteration no: 265, loss: 0.08335061961553321\n","iteration no: 266, loss: 0.07952979591955342\n","iteration no: 267, loss: 0.07588829397025039\n","iteration no: 268, loss: 0.07241532563061404\n","iteration no: 269, loss: 0.06909809286926905\n","iteration no: 270, loss: 0.06593424600813977\n","iteration no: 271, loss: 0.0629159200184\n","iteration no: 272, loss: 0.06003682885977529\n","iteration no: 273, loss: 0.057290722261959956\n","iteration no: 274, loss: 0.05467095474065562\n","iteration no: 275, loss: 0.052174578466013394\n","iteration no: 276, loss: 0.049789943965195\n","iteration no: 277, loss: 0.047514887752392765\n","iteration no: 278, loss: 0.04534436134000674\n","iteration no: 279, loss: 0.04327367607447655\n","iteration no: 280, loss: 0.041300389908781786\n","iteration no: 281, loss: 0.039415244394143\n","iteration no: 282, loss: 0.03761620210997656\n","iteration no: 283, loss: 0.03590040644983773\n","iteration no: 284, loss: 0.03426299506577882\n","iteration no: 285, loss: 0.03270084522033505\n","iteration no: 286, loss: 0.031209792950360596\n","iteration no: 287, loss: 0.029787255688444297\n","iteration no: 288, loss: 0.0284298025244253\n","iteration no: 289, loss: 0.027134139507153966\n","iteration no: 290, loss: 0.025897898827766976\n","iteration no: 291, loss: 0.024718584550702298\n","iteration no: 292, loss: 0.023595185283387762\n","iteration no: 293, loss: 0.02252116800545768\n","iteration no: 294, loss: 0.02149583164290355\n","iteration no: 295, loss: 0.020517708614322706\n","iteration no: 296, loss: 0.019584737538632498\n","iteration no: 297, loss: 0.01869413620749657\n","iteration no: 298, loss: 0.017845084809879688\n","iteration no: 299, loss: 0.017033791850599125\n","iteration no: 300, loss: 0.016259601772955896\n","iteration no: 301, loss: 0.015520592582976243\n","iteration no: 302, loss: 0.014815234004513808\n","iteration no: 303, loss: 0.014142241872170839\n","iteration no: 304, loss: 0.013500983297492634\n","iteration no: 305, loss: 0.012887865867626195\n","iteration no: 306, loss: 0.012302713135439052\n","iteration no: 307, loss: 0.011744243896101545\n","iteration no: 308, loss: 0.011211227893607081\n","iteration no: 309, loss: 0.010702433433178567\n","iteration no: 310, loss: 0.010216942850930007\n","iteration no: 311, loss: 0.009753507602229351\n","iteration no: 312, loss: 0.009311223702289883\n","iteration no: 313, loss: 0.008888898392750417\n","iteration no: 314, loss: 0.008485913694313575\n","iteration no: 315, loss: 0.008101263804229762\n","iteration no: 316, loss: 0.007734716764577348\n","iteration no: 317, loss: 0.007384154562459255\n","iteration no: 318, loss: 0.0070494915564872866\n","iteration no: 319, loss: 0.006730209431738997\n","iteration no: 320, loss: 0.006425534291142035\n","iteration no: 321, loss: 0.006134956365136435\n","iteration no: 322, loss: 0.005857085784344058\n","iteration no: 323, loss: 0.005591925296985368\n","iteration no: 324, loss: 0.005338814355997576\n","iteration no: 325, loss: 0.0050971627019799435\n","iteration no: 326, loss: 0.0048664611461157106\n","iteration no: 327, loss: 0.004646351807696359\n","iteration no: 328, loss: 0.004436412553125326\n","iteration no: 329, loss: 0.004235752399470631\n","iteration no: 330, loss: 0.004044163166145519\n","iteration no: 331, loss: 0.003861300752130683\n","iteration no: 332, loss: 0.003686712693146518\n","iteration no: 333, loss: 0.0035200197301625763\n","iteration no: 334, loss: 0.003360920374708163\n","iteration no: 335, loss: 0.003209013275635519\n","iteration no: 336, loss: 0.0030639899923268358\n","iteration no: 337, loss: 0.0029255303139723477\n","iteration no: 338, loss: 0.0027933307117358123\n","iteration no: 339, loss: 0.0026673101994900616\n","iteration no: 340, loss: 0.002546982184854477\n","iteration no: 341, loss: 0.0024319308032755404\n","iteration no: 342, loss: 0.0023220980672202917\n","iteration no: 343, loss: 0.002217396042666819\n","iteration no: 344, loss: 0.0021174135715498546\n","iteration no: 345, loss: 0.0020218155785127856\n","iteration no: 346, loss: 0.0019305589591202404\n","iteration no: 347, loss: 0.0018434374474241904\n","iteration no: 348, loss: 0.0017602629545607313\n","iteration no: 349, loss: 0.0016808227984241933\n","iteration no: 350, loss: 0.001605007283431247\n","iteration no: 351, loss: 0.0015327357699111011\n","iteration no: 352, loss: 0.0014635992335154937\n","iteration no: 353, loss: 0.0013975895527872885\n","iteration no: 354, loss: 0.0013346025525602318\n","iteration no: 355, loss: 0.0012744284345860399\n","iteration no: 356, loss: 0.0012169607257483165\n","iteration no: 357, loss: 0.0011621042933472627\n","iteration no: 358, loss: 0.0011097447296494485\n","iteration no: 359, loss: 0.0010597249330661861\n","iteration no: 360, loss: 0.0010119569800501203\n","iteration no: 361, loss: 0.0009663606155039621\n","iteration no: 362, loss: 0.000922876132004344\n","iteration no: 363, loss: 0.0008813340267698471\n","iteration no: 364, loss: 0.0008416222744535474\n","iteration no: 365, loss: 0.0008037324051788392\n","iteration no: 366, loss: 0.0007676034911922346\n","iteration no: 367, loss: 0.0007330546667304056\n","iteration no: 368, loss: 0.0007000474496325234\n","iteration no: 369, loss: 0.0006685471126557557\n","iteration no: 370, loss: 0.0006384470563711279\n","iteration no: 371, loss: 0.0006097078552348319\n","iteration no: 372, loss: 0.0005822699071653432\n","iteration no: 373, loss: 0.000556083142569346\n","iteration no: 374, loss: 0.0005310885372294303\n","iteration no: 375, loss: 0.0005071866825778245\n","iteration no: 376, loss: 0.0004843804959637465\n","iteration no: 377, loss: 0.0004625955401269122\n","iteration no: 378, loss: 0.0004417850011684721\n","iteration no: 379, loss: 0.00042190942062470384\n","iteration no: 380, loss: 0.0004029393888881874\n","iteration no: 381, loss: 0.0003848238263405203\n","iteration no: 382, loss: 0.0003675172719034943\n","iteration no: 383, loss: 0.00035099122355334965\n","iteration no: 384, loss: 0.0003352169096737671\n","iteration no: 385, loss: 0.0003201711170947691\n","iteration no: 386, loss: 0.0003057801242540171\n","iteration no: 387, loss: 0.0002920381500150666\n","iteration no: 388, loss: 0.00027893144562526765\n","iteration no: 389, loss: 0.0002664083195191916\n","iteration no: 390, loss: 0.000254439035170305\n","iteration no: 391, loss: 0.00024300865948825216\n","iteration no: 392, loss: 0.00023208926909912302\n","iteration no: 393, loss: 0.00022166316967337098\n","iteration no: 394, loss: 0.00021170661128583048\n","iteration no: 395, loss: 0.00020220378785763154\n","iteration no: 396, loss: 0.00019313404181671868\n","iteration no: 397, loss: 0.00018446517542137407\n","iteration no: 398, loss: 0.00017618294521133454\n","iteration no: 399, loss: 0.00016827548911076048\n","iteration no: 400, loss: 0.00016072029864279077\n","iteration no: 401, loss: 0.00015350624690796142\n","iteration no: 402, loss: 0.00014661518313193215\n","iteration no: 403, loss: 0.00014003448877533122\n","iteration no: 404, loss: 0.00013374952947361096\n","iteration no: 405, loss: 0.00012774867421462013\n","iteration no: 406, loss: 0.00012201759397843836\n","iteration no: 407, loss: 0.0001165488001989416\n","iteration no: 408, loss: 0.00011131902213655325\n","iteration no: 409, loss: 0.00010632705969267858\n","iteration no: 410, loss: 0.00010156025422665589\n","iteration no: 411, loss: 9.701183366130482e-05\n","iteration no: 412, loss: 9.266035245812074e-05\n","iteration no: 413, loss: 8.85051075726132e-05\n","iteration no: 414, loss: 8.45364446874998e-05\n","iteration no: 415, loss: 8.07452464473033e-05\n","iteration no: 416, loss: 7.712402045245196e-05\n","iteration no: 417, loss: 7.366917369662458e-05\n","iteration no: 418, loss: 7.0369690358314e-05\n","iteration no: 419, loss: 6.721511340874343e-05\n","iteration no: 420, loss: 6.420173503036955e-05\n","iteration no: 421, loss: 6.132480321837064e-05\n","iteration no: 422, loss: 5.8576605423339353e-05\n","iteration no: 423, loss: 5.595174103914688e-05\n","iteration no: 424, loss: 5.3443877596009436e-05\n","iteration no: 425, loss: 5.105000217876052e-05\n","iteration no: 426, loss: 4.8763069733027045e-05\n","iteration no: 427, loss: 4.657830747575239e-05\n","iteration no: 428, loss: 4.449469194652331e-05\n","iteration no: 429, loss: 4.250166478301373e-05\n","iteration no: 430, loss: 4.0597643528511056e-05\n","iteration no: 431, loss: 3.8779316482646685e-05\n","iteration no: 432, loss: 3.704317334361926e-05\n","iteration no: 433, loss: 3.538658340021226e-05\n","iteration no: 434, loss: 3.3802632251935417e-05\n","iteration no: 435, loss: 3.228883319294655e-05\n","iteration no: 436, loss: 3.084324261151062e-05\n","iteration no: 437, loss: 2.9462429299995574e-05\n","iteration no: 438, loss: 2.8144288284903484e-05\n","iteration no: 439, loss: 2.6885380934307238e-05\n","iteration no: 440, loss: 2.5681780135781845e-05\n","iteration no: 441, loss: 2.453227379949691e-05\n","iteration no: 442, loss: 2.3434300710073737e-05\n","iteration no: 443, loss: 2.238572379321229e-05\n","iteration no: 444, loss: 2.138382014620777e-05\n","iteration no: 445, loss: 2.0426802037997814e-05\n","iteration no: 446, loss: 1.951296798548961e-05\n","iteration no: 447, loss: 1.86398403050777e-05\n","iteration no: 448, loss: 1.780627184764638e-05\n","iteration no: 449, loss: 1.7010372649815663e-05\n","iteration no: 450, loss: 1.6249348398637112e-05\n","iteration no: 451, loss: 1.5522506620738445e-05\n","iteration no: 452, loss: 1.482816448210001e-05\n","iteration no: 453, loss: 1.4165421260054943e-05\n","iteration no: 454, loss: 1.3531834618360572e-05\n","iteration no: 455, loss: 1.2927530343535269e-05\n","iteration no: 456, loss: 1.234994042951238e-05\n","iteration no: 457, loss: 1.1797679947554633e-05\n","iteration no: 458, loss: 1.1270199168009781e-05\n","iteration no: 459, loss: 1.0766986074240065e-05\n","iteration no: 460, loss: 1.0285603212686915e-05\n","iteration no: 461, loss: 9.825740607514858e-06\n","iteration no: 462, loss: 9.386464429228392e-06\n","iteration no: 463, loss: 8.966966252723689e-06\n","iteration no: 464, loss: 8.566195926671689e-06\n","iteration no: 465, loss: 8.183272591946333e-06\n","iteration no: 466, loss: 7.817616208658931e-06\n","iteration no: 467, loss: 7.468214001790478e-06\n","iteration no: 468, loss: 7.134528461186507e-06\n","iteration no: 469, loss: 6.816123688956645e-06\n","iteration no: 470, loss: 6.511571178641964e-06\n","iteration no: 471, loss: 6.220614685812957e-06\n","iteration no: 472, loss: 5.9426332151667846e-06\n","iteration no: 473, loss: 5.677181863741934e-06\n","iteration no: 474, loss: 5.4236374403896795e-06\n","iteration no: 475, loss: 5.181342058275936e-06\n","iteration no: 476, loss: 4.949902912891256e-06\n","iteration no: 477, loss: 4.728968542099642e-06\n","iteration no: 478, loss: 4.51818915031516e-06\n","iteration no: 479, loss: 4.31657425163889e-06\n","iteration no: 480, loss: 4.123791827566231e-06\n","iteration no: 481, loss: 3.939691691191054e-06\n","iteration no: 482, loss: 3.763830835778753e-06\n","iteration no: 483, loss: 3.595752469830636e-06\n","iteration no: 484, loss: 3.4352081817194652e-06\n","iteration no: 485, loss: 3.281855996124518e-06\n","iteration no: 486, loss: 3.1353474506749072e-06\n","iteration no: 487, loss: 2.9953808435245508e-06\n","iteration no: 488, loss: 2.861786508544092e-06\n","iteration no: 489, loss: 2.734116442399134e-06\n","iteration no: 490, loss: 2.612065198130469e-06\n","iteration no: 491, loss: 2.495468679219216e-06\n","iteration no: 492, loss: 2.3841098368209175e-06\n","iteration no: 493, loss: 2.277701897627474e-06\n","iteration no: 494, loss: 2.1760681540835976e-06\n","iteration no: 495, loss: 2.078972484241937e-06\n","iteration no: 496, loss: 1.9862229709262265e-06\n","iteration no: 497, loss: 1.8976040862757605e-06\n","iteration no: 498, loss: 1.8130322670094185e-06\n","iteration no: 499, loss: 1.7321802840717299e-06\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f586000f7f0>]"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUGcAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Lk8T2rHDVZpH","colab_type":"code","outputId":"17ccf36c-f248-4ba9-c4cb-ecb55f6f3d09","executionInfo":{"status":"ok","timestamp":1586455857511,"user_tz":-330,"elapsed":6851,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#implementing above neural network using PyTorch\n","\n","dtype=torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","#creating random tensors\n","\n","x=torch.randn(N, D_in, dtype=dtype, device=device)\n","y=torch.randn(N, D_out, dtype=dtype, device=device)\n","w1=torch.randn(D_in, H, dtype=dtype, device=device)\n","w2=torch.randn(H, D_out, dtype=dtype, device=device)\n","\n","iterations=list()\n","costs=list()\n","\n","lr=1e-6\n","\n","for i in range(500):\n","\n","    #forward_prop\n","\n","    h=x.mm(w1)\n","    h_relu=h.clamp(min=0)\n","    y_pred=h_relu.mm(w2)\n","\n","    cost=(y_pred-y).pow(2).sum().item()\n","\n","    iterations.append(i)\n","    costs.append(cost)\n","\n","    print('iteration no: {0}, loss: {1}'.format(i, cost))\n","\n","    #back prop\n","\n","    grad_y_pred = 2*(y_pred-y)\n","    grad_w2=h_relu.t().mm(grad_y_pred)\n","    grad_h_relu=grad_y_pred.mm(w2.t())\n","    grad_h=grad_h_relu.clone()\n","    grad_h[h<0]=0\n","    grad_w1=x.t().mm(grad_h)\n","\n","    #parameter update\n","\n","    w1-=lr*grad_w1\n","    w2-=lr*grad_w2\n","\n","plt.plot(iterations, costs)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iteration no: 0, loss: 31382720.0\n","iteration no: 1, loss: 26206608.0\n","iteration no: 2, loss: 25460836.0\n","iteration no: 3, loss: 24978668.0\n","iteration no: 4, loss: 22539578.0\n","iteration no: 5, loss: 17689156.0\n","iteration no: 6, loss: 12113306.0\n","iteration no: 7, loss: 7474441.0\n","iteration no: 8, loss: 4442882.0\n","iteration no: 9, loss: 2691386.5\n","iteration no: 10, loss: 1740192.875\n","iteration no: 11, loss: 1216705.25\n","iteration no: 12, loss: 914314.5\n","iteration no: 13, loss: 725178.4375\n","iteration no: 14, loss: 596729.4375\n","iteration no: 15, loss: 502895.25\n","iteration no: 16, loss: 430327.65625\n","iteration no: 17, loss: 371926.0\n","iteration no: 18, loss: 323708.4375\n","iteration no: 19, loss: 283272.4375\n","iteration no: 20, loss: 248915.734375\n","iteration no: 21, loss: 219518.15625\n","iteration no: 22, loss: 194196.671875\n","iteration no: 23, loss: 172293.984375\n","iteration no: 24, loss: 153243.859375\n","iteration no: 25, loss: 136627.34375\n","iteration no: 26, loss: 122092.84375\n","iteration no: 27, loss: 109336.625\n","iteration no: 28, loss: 98108.1640625\n","iteration no: 29, loss: 88211.0234375\n","iteration no: 30, loss: 79452.7109375\n","iteration no: 31, loss: 71682.109375\n","iteration no: 32, loss: 64771.75\n","iteration no: 33, loss: 58615.15625\n","iteration no: 34, loss: 53117.828125\n","iteration no: 35, loss: 48201.5078125\n","iteration no: 36, loss: 43801.4140625\n","iteration no: 37, loss: 39856.22265625\n","iteration no: 38, loss: 36309.328125\n","iteration no: 39, loss: 33115.37890625\n","iteration no: 40, loss: 30234.2421875\n","iteration no: 41, loss: 27631.54296875\n","iteration no: 42, loss: 25277.853515625\n","iteration no: 43, loss: 23148.345703125\n","iteration no: 44, loss: 21217.669921875\n","iteration no: 45, loss: 19464.439453125\n","iteration no: 46, loss: 17871.845703125\n","iteration no: 47, loss: 16422.734375\n","iteration no: 48, loss: 15102.70703125\n","iteration no: 49, loss: 13900.279296875\n","iteration no: 50, loss: 12803.1337890625\n","iteration no: 51, loss: 11801.251953125\n","iteration no: 52, loss: 10885.51171875\n","iteration no: 53, loss: 10047.46875\n","iteration no: 54, loss: 9280.33203125\n","iteration no: 55, loss: 8577.224609375\n","iteration no: 56, loss: 7932.1474609375\n","iteration no: 57, loss: 7340.64892578125\n","iteration no: 58, loss: 6797.82861328125\n","iteration no: 59, loss: 6299.0791015625\n","iteration no: 60, loss: 5840.38037109375\n","iteration no: 61, loss: 5418.021484375\n","iteration no: 62, loss: 5028.9580078125\n","iteration no: 63, loss: 4671.24169921875\n","iteration no: 64, loss: 4341.39599609375\n","iteration no: 65, loss: 4036.919921875\n","iteration no: 66, loss: 3755.452880859375\n","iteration no: 67, loss: 3495.228515625\n","iteration no: 68, loss: 3254.387451171875\n","iteration no: 69, loss: 3031.47314453125\n","iteration no: 70, loss: 2825.026123046875\n","iteration no: 71, loss: 2633.74169921875\n","iteration no: 72, loss: 2456.305908203125\n","iteration no: 73, loss: 2291.724609375\n","iteration no: 74, loss: 2138.99755859375\n","iteration no: 75, loss: 1997.15234375\n","iteration no: 76, loss: 1865.381591796875\n","iteration no: 77, loss: 1742.869384765625\n","iteration no: 78, loss: 1628.9150390625\n","iteration no: 79, loss: 1522.8955078125\n","iteration no: 80, loss: 1424.2254638671875\n","iteration no: 81, loss: 1332.38720703125\n","iteration no: 82, loss: 1246.8447265625\n","iteration no: 83, loss: 1167.092041015625\n","iteration no: 84, loss: 1092.757568359375\n","iteration no: 85, loss: 1023.436279296875\n","iteration no: 86, loss: 958.7745361328125\n","iteration no: 87, loss: 898.44482421875\n","iteration no: 88, loss: 842.1260375976562\n","iteration no: 89, loss: 789.5208129882812\n","iteration no: 90, loss: 740.3736572265625\n","iteration no: 91, loss: 694.4418334960938\n","iteration no: 92, loss: 651.5211181640625\n","iteration no: 93, loss: 611.3856811523438\n","iteration no: 94, loss: 573.8449096679688\n","iteration no: 95, loss: 538.7186279296875\n","iteration no: 96, loss: 505.8415222167969\n","iteration no: 97, loss: 475.0722961425781\n","iteration no: 98, loss: 446.2631530761719\n","iteration no: 99, loss: 419.28472900390625\n","iteration no: 100, loss: 394.0099792480469\n","iteration no: 101, loss: 370.32000732421875\n","iteration no: 102, loss: 348.1145935058594\n","iteration no: 103, loss: 327.2943420410156\n","iteration no: 104, loss: 307.7757263183594\n","iteration no: 105, loss: 289.47259521484375\n","iteration no: 106, loss: 272.2972106933594\n","iteration no: 107, loss: 256.17938232421875\n","iteration no: 108, loss: 241.05099487304688\n","iteration no: 109, loss: 226.8514404296875\n","iteration no: 110, loss: 213.515380859375\n","iteration no: 111, loss: 200.99681091308594\n","iteration no: 112, loss: 189.23709106445312\n","iteration no: 113, loss: 178.19049072265625\n","iteration no: 114, loss: 167.8082275390625\n","iteration no: 115, loss: 158.0527801513672\n","iteration no: 116, loss: 148.88421630859375\n","iteration no: 117, loss: 140.2638397216797\n","iteration no: 118, loss: 132.15805053710938\n","iteration no: 119, loss: 124.53707885742188\n","iteration no: 120, loss: 117.3696517944336\n","iteration no: 121, loss: 110.62593841552734\n","iteration no: 122, loss: 104.28002166748047\n","iteration no: 123, loss: 98.3099136352539\n","iteration no: 124, loss: 92.68975067138672\n","iteration no: 125, loss: 87.40125274658203\n","iteration no: 126, loss: 82.42552185058594\n","iteration no: 127, loss: 77.73796081542969\n","iteration no: 128, loss: 73.32360076904297\n","iteration no: 129, loss: 69.16643524169922\n","iteration no: 130, loss: 65.2520980834961\n","iteration no: 131, loss: 61.564002990722656\n","iteration no: 132, loss: 58.09029006958008\n","iteration no: 133, loss: 54.81784439086914\n","iteration no: 134, loss: 51.73279571533203\n","iteration no: 135, loss: 48.82590103149414\n","iteration no: 136, loss: 46.086517333984375\n","iteration no: 137, loss: 43.50440216064453\n","iteration no: 138, loss: 41.06971740722656\n","iteration no: 139, loss: 38.774009704589844\n","iteration no: 140, loss: 36.610599517822266\n","iteration no: 141, loss: 34.56982421875\n","iteration no: 142, loss: 32.64535140991211\n","iteration no: 143, loss: 30.829832077026367\n","iteration no: 144, loss: 29.117923736572266\n","iteration no: 145, loss: 27.502925872802734\n","iteration no: 146, loss: 25.97835350036621\n","iteration no: 147, loss: 24.541311264038086\n","iteration no: 148, loss: 23.184627532958984\n","iteration no: 149, loss: 21.9036922454834\n","iteration no: 150, loss: 20.695703506469727\n","iteration no: 151, loss: 19.55543327331543\n","iteration no: 152, loss: 18.478504180908203\n","iteration no: 153, loss: 17.461843490600586\n","iteration no: 154, loss: 16.50250244140625\n","iteration no: 155, loss: 15.597496032714844\n","iteration no: 156, loss: 14.742250442504883\n","iteration no: 157, loss: 13.935150146484375\n","iteration no: 158, loss: 13.171955108642578\n","iteration no: 159, loss: 12.4518404006958\n","iteration no: 160, loss: 11.77138614654541\n","iteration no: 161, loss: 11.129084587097168\n","iteration no: 162, loss: 10.522497177124023\n","iteration no: 163, loss: 9.949262619018555\n","iteration no: 164, loss: 9.407546043395996\n","iteration no: 165, loss: 8.896309852600098\n","iteration no: 166, loss: 8.412907600402832\n","iteration no: 167, loss: 7.956094741821289\n","iteration no: 168, loss: 7.524809837341309\n","iteration no: 169, loss: 7.116360664367676\n","iteration no: 170, loss: 6.730823516845703\n","iteration no: 171, loss: 6.366859436035156\n","iteration no: 172, loss: 6.022554397583008\n","iteration no: 173, loss: 5.697332382202148\n","iteration no: 174, loss: 5.389763832092285\n","iteration no: 175, loss: 5.098596572875977\n","iteration no: 176, loss: 4.823818206787109\n","iteration no: 177, loss: 4.563764572143555\n","iteration no: 178, loss: 4.3179612159729\n","iteration no: 179, loss: 4.085758209228516\n","iteration no: 180, loss: 3.8658602237701416\n","iteration no: 181, loss: 3.6583213806152344\n","iteration no: 182, loss: 3.4616646766662598\n","iteration no: 183, loss: 3.275907516479492\n","iteration no: 184, loss: 3.1003828048706055\n","iteration no: 185, loss: 2.933967351913452\n","iteration no: 186, loss: 2.776921510696411\n","iteration no: 187, loss: 2.6283814907073975\n","iteration no: 188, loss: 2.4877870082855225\n","iteration no: 189, loss: 2.3546390533447266\n","iteration no: 190, loss: 2.229036808013916\n","iteration no: 191, loss: 2.1100475788116455\n","iteration no: 192, loss: 1.997206449508667\n","iteration no: 193, loss: 1.8906636238098145\n","iteration no: 194, loss: 1.7899649143218994\n","iteration no: 195, loss: 1.6945159435272217\n","iteration no: 196, loss: 1.6042746305465698\n","iteration no: 197, loss: 1.5190011262893677\n","iteration no: 198, loss: 1.4381709098815918\n","iteration no: 199, loss: 1.3617744445800781\n","iteration no: 200, loss: 1.2894668579101562\n","iteration no: 201, loss: 1.2209458351135254\n","iteration no: 202, loss: 1.1560815572738647\n","iteration no: 203, loss: 1.0947397947311401\n","iteration no: 204, loss: 1.0367422103881836\n","iteration no: 205, loss: 0.9817529916763306\n","iteration no: 206, loss: 0.9296854734420776\n","iteration no: 207, loss: 0.8805679082870483\n","iteration no: 208, loss: 0.833888828754425\n","iteration no: 209, loss: 0.7897638082504272\n","iteration no: 210, loss: 0.7479982972145081\n","iteration no: 211, loss: 0.7084992527961731\n","iteration no: 212, loss: 0.6711536645889282\n","iteration no: 213, loss: 0.6356109380722046\n","iteration no: 214, loss: 0.6021593809127808\n","iteration no: 215, loss: 0.5703070759773254\n","iteration no: 216, loss: 0.540252685546875\n","iteration no: 217, loss: 0.5118014812469482\n","iteration no: 218, loss: 0.484850138425827\n","iteration no: 219, loss: 0.45926544070243835\n","iteration no: 220, loss: 0.4350695013999939\n","iteration no: 221, loss: 0.41220301389694214\n","iteration no: 222, loss: 0.39051660895347595\n","iteration no: 223, loss: 0.3699573874473572\n","iteration no: 224, loss: 0.3505646586418152\n","iteration no: 225, loss: 0.33207225799560547\n","iteration no: 226, loss: 0.3146522045135498\n","iteration no: 227, loss: 0.29812026023864746\n","iteration no: 228, loss: 0.28246819972991943\n","iteration no: 229, loss: 0.26764777302742004\n","iteration no: 230, loss: 0.2536289393901825\n","iteration no: 231, loss: 0.24036943912506104\n","iteration no: 232, loss: 0.22778132557868958\n","iteration no: 233, loss: 0.21581020951271057\n","iteration no: 234, loss: 0.20453503727912903\n","iteration no: 235, loss: 0.19380870461463928\n","iteration no: 236, loss: 0.183699369430542\n","iteration no: 237, loss: 0.1740809679031372\n","iteration no: 238, loss: 0.1649729311466217\n","iteration no: 239, loss: 0.1563871204853058\n","iteration no: 240, loss: 0.14819619059562683\n","iteration no: 241, loss: 0.14045783877372742\n","iteration no: 242, loss: 0.13312454521656036\n","iteration no: 243, loss: 0.12614625692367554\n","iteration no: 244, loss: 0.11961181461811066\n","iteration no: 245, loss: 0.11336349695920944\n","iteration no: 246, loss: 0.10744954645633698\n","iteration no: 247, loss: 0.10185670852661133\n","iteration no: 248, loss: 0.09655492007732391\n","iteration no: 249, loss: 0.09152095764875412\n","iteration no: 250, loss: 0.08674225956201553\n","iteration no: 251, loss: 0.08221219480037689\n","iteration no: 252, loss: 0.07794372737407684\n","iteration no: 253, loss: 0.07389534264802933\n","iteration no: 254, loss: 0.07005858421325684\n","iteration no: 255, loss: 0.06642213463783264\n","iteration no: 256, loss: 0.06296825408935547\n","iteration no: 257, loss: 0.05970268324017525\n","iteration no: 258, loss: 0.0565975084900856\n","iteration no: 259, loss: 0.05364621803164482\n","iteration no: 260, loss: 0.05086187273263931\n","iteration no: 261, loss: 0.04823172092437744\n","iteration no: 262, loss: 0.04570959508419037\n","iteration no: 263, loss: 0.043343380093574524\n","iteration no: 264, loss: 0.04110598936676979\n","iteration no: 265, loss: 0.03898171707987785\n","iteration no: 266, loss: 0.03700004518032074\n","iteration no: 267, loss: 0.03506853058934212\n","iteration no: 268, loss: 0.033259205520153046\n","iteration no: 269, loss: 0.031533703207969666\n","iteration no: 270, loss: 0.029901832342147827\n","iteration no: 271, loss: 0.028357800096273422\n","iteration no: 272, loss: 0.026905905455350876\n","iteration no: 273, loss: 0.025517528876662254\n","iteration no: 274, loss: 0.024200377985835075\n","iteration no: 275, loss: 0.0229492150247097\n","iteration no: 276, loss: 0.02176601253449917\n","iteration no: 277, loss: 0.02064487524330616\n","iteration no: 278, loss: 0.019584644585847855\n","iteration no: 279, loss: 0.018590174615383148\n","iteration no: 280, loss: 0.017632484436035156\n","iteration no: 281, loss: 0.01672595739364624\n","iteration no: 282, loss: 0.015866750851273537\n","iteration no: 283, loss: 0.015058894641697407\n","iteration no: 284, loss: 0.014296358451247215\n","iteration no: 285, loss: 0.01357139740139246\n","iteration no: 286, loss: 0.012873593717813492\n","iteration no: 287, loss: 0.012228135950863361\n","iteration no: 288, loss: 0.011609917506575584\n","iteration no: 289, loss: 0.011022479273378849\n","iteration no: 290, loss: 0.010467051528394222\n","iteration no: 291, loss: 0.009944254532456398\n","iteration no: 292, loss: 0.00945068709552288\n","iteration no: 293, loss: 0.008977681398391724\n","iteration no: 294, loss: 0.008524276316165924\n","iteration no: 295, loss: 0.008098500780761242\n","iteration no: 296, loss: 0.007694342639297247\n","iteration no: 297, loss: 0.007309178356081247\n","iteration no: 298, loss: 0.006947268731892109\n","iteration no: 299, loss: 0.006603333167731762\n","iteration no: 300, loss: 0.006275468971580267\n","iteration no: 301, loss: 0.005966105032712221\n","iteration no: 302, loss: 0.005671813152730465\n","iteration no: 303, loss: 0.005387176759541035\n","iteration no: 304, loss: 0.005128840450197458\n","iteration no: 305, loss: 0.004882940091192722\n","iteration no: 306, loss: 0.0046439445577561855\n","iteration no: 307, loss: 0.004421767313033342\n","iteration no: 308, loss: 0.004212705418467522\n","iteration no: 309, loss: 0.004008185118436813\n","iteration no: 310, loss: 0.0038163922727108\n","iteration no: 311, loss: 0.003635603003203869\n","iteration no: 312, loss: 0.003463142551481724\n","iteration no: 313, loss: 0.0032996658701449633\n","iteration no: 314, loss: 0.0031474758870899677\n","iteration no: 315, loss: 0.003002104815095663\n","iteration no: 316, loss: 0.0028609924484044313\n","iteration no: 317, loss: 0.0027287835255265236\n","iteration no: 318, loss: 0.0026024957187473774\n","iteration no: 319, loss: 0.0024808531161397696\n","iteration no: 320, loss: 0.002369660884141922\n","iteration no: 321, loss: 0.0022614996414631605\n","iteration no: 322, loss: 0.0021622944623231888\n","iteration no: 323, loss: 0.0020649272482842207\n","iteration no: 324, loss: 0.0019743714947253466\n","iteration no: 325, loss: 0.0018867215840145946\n","iteration no: 326, loss: 0.0018052703235298395\n","iteration no: 327, loss: 0.001723077381029725\n","iteration no: 328, loss: 0.0016500873025506735\n","iteration no: 329, loss: 0.0015813854988664389\n","iteration no: 330, loss: 0.001513417810201645\n","iteration no: 331, loss: 0.001448139431886375\n","iteration no: 332, loss: 0.0013855424476787448\n","iteration no: 333, loss: 0.0013269796036183834\n","iteration no: 334, loss: 0.0012722614919766784\n","iteration no: 335, loss: 0.001220356673002243\n","iteration no: 336, loss: 0.0011701243929564953\n","iteration no: 337, loss: 0.0011218484723940492\n","iteration no: 338, loss: 0.001076968153938651\n","iteration no: 339, loss: 0.0010357600403949618\n","iteration no: 340, loss: 0.0009952826658263803\n","iteration no: 341, loss: 0.000954443879891187\n","iteration no: 342, loss: 0.0009188993717543781\n","iteration no: 343, loss: 0.0008836204651743174\n","iteration no: 344, loss: 0.0008485183352604508\n","iteration no: 345, loss: 0.0008176678093150258\n","iteration no: 346, loss: 0.0007839571917429566\n","iteration no: 347, loss: 0.0007560836966149509\n","iteration no: 348, loss: 0.0007270756759680808\n","iteration no: 349, loss: 0.0007007077219896019\n","iteration no: 350, loss: 0.0006743245758116245\n","iteration no: 351, loss: 0.0006495323614217341\n","iteration no: 352, loss: 0.0006259457441046834\n","iteration no: 353, loss: 0.0006027433555573225\n","iteration no: 354, loss: 0.0005816036718897521\n","iteration no: 355, loss: 0.000561435823328793\n","iteration no: 356, loss: 0.0005421845125965774\n","iteration no: 357, loss: 0.000523168477229774\n","iteration no: 358, loss: 0.0005056076333858073\n","iteration no: 359, loss: 0.00048792315647006035\n","iteration no: 360, loss: 0.0004704768070951104\n","iteration no: 361, loss: 0.00045436376240104437\n","iteration no: 362, loss: 0.00043952444684691727\n","iteration no: 363, loss: 0.00042515917448326945\n","iteration no: 364, loss: 0.0004117120406590402\n","iteration no: 365, loss: 0.0003981506160926074\n","iteration no: 366, loss: 0.0003849677159450948\n","iteration no: 367, loss: 0.00037244544364511967\n","iteration no: 368, loss: 0.00036066753091290593\n","iteration no: 369, loss: 0.00034967396641150117\n","iteration no: 370, loss: 0.0003388317418284714\n","iteration no: 371, loss: 0.0003284354170318693\n","iteration no: 372, loss: 0.00031898883753456175\n","iteration no: 373, loss: 0.00030899306875653565\n","iteration no: 374, loss: 0.0002998050185851753\n","iteration no: 375, loss: 0.00029057078063488007\n","iteration no: 376, loss: 0.0002812784514389932\n","iteration no: 377, loss: 0.0002734852605499327\n","iteration no: 378, loss: 0.0002647870278451592\n","iteration no: 379, loss: 0.0002578228304628283\n","iteration no: 380, loss: 0.0002503175346646458\n","iteration no: 381, loss: 0.00024338213552255183\n","iteration no: 382, loss: 0.0002359794161748141\n","iteration no: 383, loss: 0.00022911932319402695\n","iteration no: 384, loss: 0.0002231744583696127\n","iteration no: 385, loss: 0.0002163759636459872\n","iteration no: 386, loss: 0.00021081077284179628\n","iteration no: 387, loss: 0.00020542557467706501\n","iteration no: 388, loss: 0.00019991875160485506\n","iteration no: 389, loss: 0.00019431411055848002\n","iteration no: 390, loss: 0.0001888977421913296\n","iteration no: 391, loss: 0.0001842481578933075\n","iteration no: 392, loss: 0.00017952756024897099\n","iteration no: 393, loss: 0.00017461333482060581\n","iteration no: 394, loss: 0.0001701909932307899\n","iteration no: 395, loss: 0.00016616462380625308\n","iteration no: 396, loss: 0.00016193735064007342\n","iteration no: 397, loss: 0.00015768325829412788\n","iteration no: 398, loss: 0.00015373789938166738\n","iteration no: 399, loss: 0.00015028929919935763\n","iteration no: 400, loss: 0.00014666543575003743\n","iteration no: 401, loss: 0.00014322606148198247\n","iteration no: 402, loss: 0.00013939585187472403\n","iteration no: 403, loss: 0.00013604268315248191\n","iteration no: 404, loss: 0.00013289801427163184\n","iteration no: 405, loss: 0.000129501277115196\n","iteration no: 406, loss: 0.00012663673260249197\n","iteration no: 407, loss: 0.00012375823280308396\n","iteration no: 408, loss: 0.00012024026364088058\n","iteration no: 409, loss: 0.00011763400107156485\n","iteration no: 410, loss: 0.00011496079969219863\n","iteration no: 411, loss: 0.00011258797167101875\n","iteration no: 412, loss: 0.00010985870903823525\n","iteration no: 413, loss: 0.00010751053923740983\n","iteration no: 414, loss: 0.00010536044283071533\n","iteration no: 415, loss: 0.00010302757436875254\n","iteration no: 416, loss: 0.00010078777268063277\n","iteration no: 417, loss: 9.877052798401564e-05\n","iteration no: 418, loss: 9.653459710534662e-05\n","iteration no: 419, loss: 9.436238906346262e-05\n","iteration no: 420, loss: 9.224463428836316e-05\n","iteration no: 421, loss: 9.073651017388329e-05\n","iteration no: 422, loss: 8.85684639797546e-05\n","iteration no: 423, loss: 8.649102528579533e-05\n","iteration no: 424, loss: 8.487033483106643e-05\n","iteration no: 425, loss: 8.316151797771454e-05\n","iteration no: 426, loss: 8.126714965328574e-05\n","iteration no: 427, loss: 7.964794349391013e-05\n","iteration no: 428, loss: 7.812518015271053e-05\n","iteration no: 429, loss: 7.648910104762763e-05\n","iteration no: 430, loss: 7.497596379835159e-05\n","iteration no: 431, loss: 7.332365203183144e-05\n","iteration no: 432, loss: 7.192290649982169e-05\n","iteration no: 433, loss: 7.055167952785268e-05\n","iteration no: 434, loss: 6.946649955352768e-05\n","iteration no: 435, loss: 6.805345765314996e-05\n","iteration no: 436, loss: 6.665858381893486e-05\n","iteration no: 437, loss: 6.553973071277142e-05\n","iteration no: 438, loss: 6.4071238739416e-05\n","iteration no: 439, loss: 6.277932698139921e-05\n","iteration no: 440, loss: 6.18883132119663e-05\n","iteration no: 441, loss: 6.077711077523418e-05\n","iteration no: 442, loss: 5.975404928904027e-05\n","iteration no: 443, loss: 5.8548685046844184e-05\n","iteration no: 444, loss: 5.754037556471303e-05\n","iteration no: 445, loss: 5.647022044286132e-05\n","iteration no: 446, loss: 5.536446406040341e-05\n","iteration no: 447, loss: 5.440159293357283e-05\n","iteration no: 448, loss: 5.347483966033906e-05\n","iteration no: 449, loss: 5.252697883406654e-05\n","iteration no: 450, loss: 5.154356040293351e-05\n","iteration no: 451, loss: 5.066437734058127e-05\n","iteration no: 452, loss: 5.000481178285554e-05\n","iteration no: 453, loss: 4.910164716420695e-05\n","iteration no: 454, loss: 4.801950854016468e-05\n","iteration no: 455, loss: 4.7287587221944705e-05\n","iteration no: 456, loss: 4.657792305806652e-05\n","iteration no: 457, loss: 4.574968625092879e-05\n","iteration no: 458, loss: 4.518954665400088e-05\n","iteration no: 459, loss: 4.446774983080104e-05\n","iteration no: 460, loss: 4.375270873424597e-05\n","iteration no: 461, loss: 4.307499330025166e-05\n","iteration no: 462, loss: 4.240783164277673e-05\n","iteration no: 463, loss: 4.161486140219495e-05\n","iteration no: 464, loss: 4.117693970329128e-05\n","iteration no: 465, loss: 4.0302518755197525e-05\n","iteration no: 466, loss: 3.959083915106021e-05\n","iteration no: 467, loss: 3.906589699909091e-05\n","iteration no: 468, loss: 3.8328496884787455e-05\n","iteration no: 469, loss: 3.7847763451281935e-05\n","iteration no: 470, loss: 3.7245292332954705e-05\n","iteration no: 471, loss: 3.665856274892576e-05\n","iteration no: 472, loss: 3.627555997809395e-05\n","iteration no: 473, loss: 3.577180905267596e-05\n","iteration no: 474, loss: 3.534698043949902e-05\n","iteration no: 475, loss: 3.477850987110287e-05\n","iteration no: 476, loss: 3.424385067773983e-05\n","iteration no: 477, loss: 3.383236253284849e-05\n","iteration no: 478, loss: 3.324988210806623e-05\n","iteration no: 479, loss: 3.280375312897377e-05\n","iteration no: 480, loss: 3.2421819923911244e-05\n","iteration no: 481, loss: 3.1898649467621e-05\n","iteration no: 482, loss: 3.149260737700388e-05\n","iteration no: 483, loss: 3.1029452657094225e-05\n","iteration no: 484, loss: 3.0531318770954385e-05\n","iteration no: 485, loss: 3.0188937671482563e-05\n","iteration no: 486, loss: 2.9859646019758657e-05\n","iteration no: 487, loss: 2.950696216430515e-05\n","iteration no: 488, loss: 2.9123211788828485e-05\n","iteration no: 489, loss: 2.868065348593518e-05\n","iteration no: 490, loss: 2.83337794826366e-05\n","iteration no: 491, loss: 2.7937974664382637e-05\n","iteration no: 492, loss: 2.7591706384555437e-05\n","iteration no: 493, loss: 2.713128924369812e-05\n","iteration no: 494, loss: 2.6837389668799005e-05\n","iteration no: 495, loss: 2.6475796403246932e-05\n","iteration no: 496, loss: 2.620831219246611e-05\n","iteration no: 497, loss: 2.5853911211015657e-05\n","iteration no: 498, loss: 2.5506786187179387e-05\n","iteration no: 499, loss: 2.524140290915966e-05\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f585fafc4e0>]"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEf8fyXOBy1NdEEgAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"qY1DimCHdpA0","colab_type":"code","outputId":"4fb212b0-41c2-4379-9049-a0c40d14c2fa","executionInfo":{"status":"ok","timestamp":1586455862419,"user_tz":-330,"elapsed":11259,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#same network using auto grad\n","\n","dtype=torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","#creating random tensors\n","\n","x=torch.randn(N, D_in, dtype=dtype, device=device)\n","y=torch.randn(N, D_out, dtype=dtype, device=device)\n","w1=torch.randn(D_in, H, dtype=dtype, device=device, requires_grad=True)\n","w2=torch.randn(H, D_out, dtype=dtype, device=device, requires_grad=True)\n","\n","iterations=list()\n","costs=list()\n","\n","lr=1e-6\n","\n","for i in range(500):\n","    y_pred=(x.mm(w1).clamp(min=0)).mm(w2)\n","\n","    cost=(y_pred-y).pow(2).sum()\n","\n","    iterations.append(i)\n","    costs.append(cost.item())\n","\n","    print('iteration no: {0}, loss: {1}'.format(i, cost.item()))\n","\n","    cost.backward()\n","\n","    #update params\n","    with torch.no_grad():\n","        w1-=lr*(w1.grad)\n","        w2-=lr*(w2.grad)\n","\n","    w1.grad.zero_()\n","    w2.grad.zero_()\n","\n","plt.plot(iterations, costs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iteration no: 0, loss: 37649384.0\n","iteration no: 1, loss: 37519260.0\n","iteration no: 2, loss: 39444132.0\n","iteration no: 3, loss: 35762232.0\n","iteration no: 4, loss: 25252920.0\n","iteration no: 5, loss: 13778731.0\n","iteration no: 6, loss: 6603060.0\n","iteration no: 7, loss: 3321234.25\n","iteration no: 8, loss: 1969950.125\n","iteration no: 9, loss: 1371814.75\n","iteration no: 10, loss: 1057304.375\n","iteration no: 11, loss: 858754.5\n","iteration no: 12, loss: 716233.125\n","iteration no: 13, loss: 606338.375\n","iteration no: 14, loss: 518341.78125\n","iteration no: 15, loss: 446401.0625\n","iteration no: 16, loss: 386825.34375\n","iteration no: 17, loss: 336991.5625\n","iteration no: 18, loss: 295009.34375\n","iteration no: 19, loss: 259448.078125\n","iteration no: 20, loss: 229138.5625\n","iteration no: 21, loss: 203156.875\n","iteration no: 22, loss: 180775.09375\n","iteration no: 23, loss: 161434.640625\n","iteration no: 24, loss: 144633.9375\n","iteration no: 25, loss: 130004.359375\n","iteration no: 26, loss: 117190.3125\n","iteration no: 27, loss: 105920.5390625\n","iteration no: 28, loss: 95974.0078125\n","iteration no: 29, loss: 87171.0\n","iteration no: 30, loss: 79369.59375\n","iteration no: 31, loss: 72419.3125\n","iteration no: 32, loss: 66211.0078125\n","iteration no: 33, loss: 60646.80078125\n","iteration no: 34, loss: 55651.984375\n","iteration no: 35, loss: 51154.25390625\n","iteration no: 36, loss: 47095.4921875\n","iteration no: 37, loss: 43425.27734375\n","iteration no: 38, loss: 40101.08984375\n","iteration no: 39, loss: 37080.80859375\n","iteration no: 40, loss: 34333.74609375\n","iteration no: 41, loss: 31832.9921875\n","iteration no: 42, loss: 29550.330078125\n","iteration no: 43, loss: 27463.0546875\n","iteration no: 44, loss: 25550.4375\n","iteration no: 45, loss: 23795.5859375\n","iteration no: 46, loss: 22184.001953125\n","iteration no: 47, loss: 20701.181640625\n","iteration no: 48, loss: 19335.166015625\n","iteration no: 49, loss: 18075.193359375\n","iteration no: 50, loss: 16912.97265625\n","iteration no: 51, loss: 15837.783203125\n","iteration no: 52, loss: 14841.654296875\n","iteration no: 53, loss: 13918.390625\n","iteration no: 54, loss: 13062.3310546875\n","iteration no: 55, loss: 12267.2587890625\n","iteration no: 56, loss: 11528.1689453125\n","iteration no: 57, loss: 10841.123046875\n","iteration no: 58, loss: 10201.0458984375\n","iteration no: 59, loss: 9604.1650390625\n","iteration no: 60, loss: 9047.1865234375\n","iteration no: 61, loss: 8527.4111328125\n","iteration no: 62, loss: 8042.43115234375\n","iteration no: 63, loss: 7589.0888671875\n","iteration no: 64, loss: 7164.978515625\n","iteration no: 65, loss: 6767.64599609375\n","iteration no: 66, loss: 6395.12939453125\n","iteration no: 67, loss: 6045.9052734375\n","iteration no: 68, loss: 5718.3662109375\n","iteration no: 69, loss: 5411.2705078125\n","iteration no: 70, loss: 5122.501953125\n","iteration no: 71, loss: 4851.1142578125\n","iteration no: 72, loss: 4595.9892578125\n","iteration no: 73, loss: 4355.75\n","iteration no: 74, loss: 4129.6513671875\n","iteration no: 75, loss: 3916.830322265625\n","iteration no: 76, loss: 3716.239013671875\n","iteration no: 77, loss: 3527.138671875\n","iteration no: 78, loss: 3348.68798828125\n","iteration no: 79, loss: 3180.2275390625\n","iteration no: 80, loss: 3021.296142578125\n","iteration no: 81, loss: 2871.1142578125\n","iteration no: 82, loss: 2729.153076171875\n","iteration no: 83, loss: 2594.92041015625\n","iteration no: 84, loss: 2468.04638671875\n","iteration no: 85, loss: 2348.142578125\n","iteration no: 86, loss: 2234.666259765625\n","iteration no: 87, loss: 2127.33251953125\n","iteration no: 88, loss: 2025.6060791015625\n","iteration no: 89, loss: 1929.1806640625\n","iteration no: 90, loss: 1837.812255859375\n","iteration no: 91, loss: 1751.273681640625\n","iteration no: 92, loss: 1669.0975341796875\n","iteration no: 93, loss: 1591.163818359375\n","iteration no: 94, loss: 1517.1900634765625\n","iteration no: 95, loss: 1446.98828125\n","iteration no: 96, loss: 1380.301513671875\n","iteration no: 97, loss: 1316.9442138671875\n","iteration no: 98, loss: 1256.7279052734375\n","iteration no: 99, loss: 1199.545166015625\n","iteration no: 100, loss: 1145.19580078125\n","iteration no: 101, loss: 1093.5740966796875\n","iteration no: 102, loss: 1044.4737548828125\n","iteration no: 103, loss: 997.7607421875\n","iteration no: 104, loss: 953.323974609375\n","iteration no: 105, loss: 911.0206909179688\n","iteration no: 106, loss: 870.7295532226562\n","iteration no: 107, loss: 832.3392944335938\n","iteration no: 108, loss: 795.7894897460938\n","iteration no: 109, loss: 760.9586791992188\n","iteration no: 110, loss: 727.7578735351562\n","iteration no: 111, loss: 696.1470947265625\n","iteration no: 112, loss: 665.9992065429688\n","iteration no: 113, loss: 637.2301025390625\n","iteration no: 114, loss: 609.7928466796875\n","iteration no: 115, loss: 583.6235961914062\n","iteration no: 116, loss: 558.6649169921875\n","iteration no: 117, loss: 534.8548583984375\n","iteration no: 118, loss: 512.1015014648438\n","iteration no: 119, loss: 490.39837646484375\n","iteration no: 120, loss: 469.6727294921875\n","iteration no: 121, loss: 449.8718566894531\n","iteration no: 122, loss: 430.9598083496094\n","iteration no: 123, loss: 412.899658203125\n","iteration no: 124, loss: 395.6318054199219\n","iteration no: 125, loss: 379.1365966796875\n","iteration no: 126, loss: 363.376953125\n","iteration no: 127, loss: 348.29852294921875\n","iteration no: 128, loss: 333.9019470214844\n","iteration no: 129, loss: 320.1372375488281\n","iteration no: 130, loss: 306.9522705078125\n","iteration no: 131, loss: 294.34375\n","iteration no: 132, loss: 282.2811279296875\n","iteration no: 133, loss: 270.7513122558594\n","iteration no: 134, loss: 259.71221923828125\n","iteration no: 135, loss: 249.1494903564453\n","iteration no: 136, loss: 239.03753662109375\n","iteration no: 137, loss: 229.35264587402344\n","iteration no: 138, loss: 220.0863494873047\n","iteration no: 139, loss: 211.21690368652344\n","iteration no: 140, loss: 202.72283935546875\n","iteration no: 141, loss: 194.5846710205078\n","iteration no: 142, loss: 186.7853240966797\n","iteration no: 143, loss: 179.31350708007812\n","iteration no: 144, loss: 172.16171264648438\n","iteration no: 145, loss: 165.30328369140625\n","iteration no: 146, loss: 158.7333221435547\n","iteration no: 147, loss: 152.44192504882812\n","iteration no: 148, loss: 146.40240478515625\n","iteration no: 149, loss: 140.61407470703125\n","iteration no: 150, loss: 135.07080078125\n","iteration no: 151, loss: 129.74969482421875\n","iteration no: 152, loss: 124.65261840820312\n","iteration no: 153, loss: 119.75947570800781\n","iteration no: 154, loss: 115.06773376464844\n","iteration no: 155, loss: 110.56904602050781\n","iteration no: 156, loss: 106.2514419555664\n","iteration no: 157, loss: 102.11028289794922\n","iteration no: 158, loss: 98.14038848876953\n","iteration no: 159, loss: 94.32502746582031\n","iteration no: 160, loss: 90.66842651367188\n","iteration no: 161, loss: 87.15677642822266\n","iteration no: 162, loss: 83.7861557006836\n","iteration no: 163, loss: 80.55281066894531\n","iteration no: 164, loss: 77.44674682617188\n","iteration no: 165, loss: 74.46566772460938\n","iteration no: 166, loss: 71.60411834716797\n","iteration no: 167, loss: 68.85562133789062\n","iteration no: 168, loss: 66.21617126464844\n","iteration no: 169, loss: 63.68387985229492\n","iteration no: 170, loss: 61.25274658203125\n","iteration no: 171, loss: 58.913055419921875\n","iteration no: 172, loss: 56.667152404785156\n","iteration no: 173, loss: 54.508548736572266\n","iteration no: 174, loss: 52.43688201904297\n","iteration no: 175, loss: 50.44802474975586\n","iteration no: 176, loss: 48.53396224975586\n","iteration no: 177, loss: 46.69579315185547\n","iteration no: 178, loss: 44.92881774902344\n","iteration no: 179, loss: 43.23271179199219\n","iteration no: 180, loss: 41.602447509765625\n","iteration no: 181, loss: 40.03411102294922\n","iteration no: 182, loss: 38.526947021484375\n","iteration no: 183, loss: 37.077083587646484\n","iteration no: 184, loss: 35.68478775024414\n","iteration no: 185, loss: 34.34614181518555\n","iteration no: 186, loss: 33.06041717529297\n","iteration no: 187, loss: 31.8215389251709\n","iteration no: 188, loss: 30.63292121887207\n","iteration no: 189, loss: 29.489002227783203\n","iteration no: 190, loss: 28.388851165771484\n","iteration no: 191, loss: 27.33156967163086\n","iteration no: 192, loss: 26.313236236572266\n","iteration no: 193, loss: 25.333837509155273\n","iteration no: 194, loss: 24.39291763305664\n","iteration no: 195, loss: 23.486541748046875\n","iteration no: 196, loss: 22.61585807800293\n","iteration no: 197, loss: 21.777957916259766\n","iteration no: 198, loss: 20.972450256347656\n","iteration no: 199, loss: 20.196487426757812\n","iteration no: 200, loss: 19.449996948242188\n","iteration no: 201, loss: 18.731689453125\n","iteration no: 202, loss: 18.04146957397461\n","iteration no: 203, loss: 17.376419067382812\n","iteration no: 204, loss: 16.73649787902832\n","iteration no: 205, loss: 16.120834350585938\n","iteration no: 206, loss: 15.52914047241211\n","iteration no: 207, loss: 14.95904541015625\n","iteration no: 208, loss: 14.410215377807617\n","iteration no: 209, loss: 13.881829261779785\n","iteration no: 210, loss: 13.373205184936523\n","iteration no: 211, loss: 12.88323974609375\n","iteration no: 212, loss: 12.411799430847168\n","iteration no: 213, loss: 11.958505630493164\n","iteration no: 214, loss: 11.522048950195312\n","iteration no: 215, loss: 11.102201461791992\n","iteration no: 216, loss: 10.697315216064453\n","iteration no: 217, loss: 10.3076171875\n","iteration no: 218, loss: 9.932000160217285\n","iteration no: 219, loss: 9.570793151855469\n","iteration no: 220, loss: 9.222613334655762\n","iteration no: 221, loss: 8.887421607971191\n","iteration no: 222, loss: 8.564453125\n","iteration no: 223, loss: 8.254294395446777\n","iteration no: 224, loss: 7.955099582672119\n","iteration no: 225, loss: 7.666638374328613\n","iteration no: 226, loss: 7.38905143737793\n","iteration no: 227, loss: 7.121670722961426\n","iteration no: 228, loss: 6.863980293273926\n","iteration no: 229, loss: 6.616215229034424\n","iteration no: 230, loss: 6.376852989196777\n","iteration no: 231, loss: 6.147177219390869\n","iteration no: 232, loss: 5.925695419311523\n","iteration no: 233, loss: 5.71198844909668\n","iteration no: 234, loss: 5.505953788757324\n","iteration no: 235, loss: 5.307930946350098\n","iteration no: 236, loss: 5.117004871368408\n","iteration no: 237, loss: 4.932727813720703\n","iteration no: 238, loss: 4.755354881286621\n","iteration no: 239, loss: 4.5849175453186035\n","iteration no: 240, loss: 4.420315742492676\n","iteration no: 241, loss: 4.261539936065674\n","iteration no: 242, loss: 4.1087260246276855\n","iteration no: 243, loss: 3.9614834785461426\n","iteration no: 244, loss: 3.8196558952331543\n","iteration no: 245, loss: 3.6828060150146484\n","iteration no: 246, loss: 3.5508391857147217\n","iteration no: 247, loss: 3.4240026473999023\n","iteration no: 248, loss: 3.301389694213867\n","iteration no: 249, loss: 3.183614730834961\n","iteration no: 250, loss: 3.0698723793029785\n","iteration no: 251, loss: 2.960221290588379\n","iteration no: 252, loss: 2.8547444343566895\n","iteration no: 253, loss: 2.7528395652770996\n","iteration no: 254, loss: 2.654688835144043\n","iteration no: 255, loss: 2.5602824687957764\n","iteration no: 256, loss: 2.4691028594970703\n","iteration no: 257, loss: 2.3811752796173096\n","iteration no: 258, loss: 2.2963907718658447\n","iteration no: 259, loss: 2.2148966789245605\n","iteration no: 260, loss: 2.1361663341522217\n","iteration no: 261, loss: 2.0602076053619385\n","iteration no: 262, loss: 1.98715341091156\n","iteration no: 263, loss: 1.9164539575576782\n","iteration no: 264, loss: 1.8485052585601807\n","iteration no: 265, loss: 1.783069133758545\n","iteration no: 266, loss: 1.7198222875595093\n","iteration no: 267, loss: 1.6588149070739746\n","iteration no: 268, loss: 1.6000429391860962\n","iteration no: 269, loss: 1.5433924198150635\n","iteration no: 270, loss: 1.488871455192566\n","iteration no: 271, loss: 1.4361392259597778\n","iteration no: 272, loss: 1.385266661643982\n","iteration no: 273, loss: 1.3363233804702759\n","iteration no: 274, loss: 1.2892650365829468\n","iteration no: 275, loss: 1.243617057800293\n","iteration no: 276, loss: 1.1997084617614746\n","iteration no: 277, loss: 1.1573485136032104\n","iteration no: 278, loss: 1.1165575981140137\n","iteration no: 279, loss: 1.077059030532837\n","iteration no: 280, loss: 1.0391199588775635\n","iteration no: 281, loss: 1.0025237798690796\n","iteration no: 282, loss: 0.9671412706375122\n","iteration no: 283, loss: 0.9330939054489136\n","iteration no: 284, loss: 0.9001808166503906\n","iteration no: 285, loss: 0.8686351776123047\n","iteration no: 286, loss: 0.838045060634613\n","iteration no: 287, loss: 0.8085529208183289\n","iteration no: 288, loss: 0.7801159024238586\n","iteration no: 289, loss: 0.7526861429214478\n","iteration no: 290, loss: 0.7262338399887085\n","iteration no: 291, loss: 0.7007424235343933\n","iteration no: 292, loss: 0.6760592460632324\n","iteration no: 293, loss: 0.6523770093917847\n","iteration no: 294, loss: 0.6294811964035034\n","iteration no: 295, loss: 0.6073724627494812\n","iteration no: 296, loss: 0.5860979557037354\n","iteration no: 297, loss: 0.5654793977737427\n","iteration no: 298, loss: 0.5455958843231201\n","iteration no: 299, loss: 0.5265241265296936\n","iteration no: 300, loss: 0.5080304741859436\n","iteration no: 301, loss: 0.4903300702571869\n","iteration no: 302, loss: 0.473158597946167\n","iteration no: 303, loss: 0.45660755038261414\n","iteration no: 304, loss: 0.4406208097934723\n","iteration no: 305, loss: 0.4251415431499481\n","iteration no: 306, loss: 0.4103280305862427\n","iteration no: 307, loss: 0.39600276947021484\n","iteration no: 308, loss: 0.3821176588535309\n","iteration no: 309, loss: 0.3687725365161896\n","iteration no: 310, loss: 0.3558683693408966\n","iteration no: 311, loss: 0.34346580505371094\n","iteration no: 312, loss: 0.3314761519432068\n","iteration no: 313, loss: 0.31985795497894287\n","iteration no: 314, loss: 0.30871689319610596\n","iteration no: 315, loss: 0.2979786992073059\n","iteration no: 316, loss: 0.2875317335128784\n","iteration no: 317, loss: 0.27751150727272034\n","iteration no: 318, loss: 0.2678447663784027\n","iteration no: 319, loss: 0.25853049755096436\n","iteration no: 320, loss: 0.24946340918540955\n","iteration no: 321, loss: 0.24082937836647034\n","iteration no: 322, loss: 0.23239564895629883\n","iteration no: 323, loss: 0.22429220378398895\n","iteration no: 324, loss: 0.21648845076560974\n","iteration no: 325, loss: 0.2089289128780365\n","iteration no: 326, loss: 0.20170925557613373\n","iteration no: 327, loss: 0.1946394145488739\n","iteration no: 328, loss: 0.18788298964500427\n","iteration no: 329, loss: 0.18136045336723328\n","iteration no: 330, loss: 0.1750693917274475\n","iteration no: 331, loss: 0.16899320483207703\n","iteration no: 332, loss: 0.163080096244812\n","iteration no: 333, loss: 0.15740475058555603\n","iteration no: 334, loss: 0.1519870162010193\n","iteration no: 335, loss: 0.14666931331157684\n","iteration no: 336, loss: 0.1415984332561493\n","iteration no: 337, loss: 0.13668663799762726\n","iteration no: 338, loss: 0.13195443153381348\n","iteration no: 339, loss: 0.12736228108406067\n","iteration no: 340, loss: 0.1229415163397789\n","iteration no: 341, loss: 0.11864053457975388\n","iteration no: 342, loss: 0.11456552147865295\n","iteration no: 343, loss: 0.11057794839143753\n","iteration no: 344, loss: 0.1067328155040741\n","iteration no: 345, loss: 0.10306321084499359\n","iteration no: 346, loss: 0.09949170053005219\n","iteration no: 347, loss: 0.09604404866695404\n","iteration no: 348, loss: 0.09271997958421707\n","iteration no: 349, loss: 0.08950281888246536\n","iteration no: 350, loss: 0.08644592761993408\n","iteration no: 351, loss: 0.08342758566141129\n","iteration no: 352, loss: 0.08056391775608063\n","iteration no: 353, loss: 0.07775259763002396\n","iteration no: 354, loss: 0.07507246732711792\n","iteration no: 355, loss: 0.0724732056260109\n","iteration no: 356, loss: 0.06996387988328934\n","iteration no: 357, loss: 0.06755368411540985\n","iteration no: 358, loss: 0.06521782279014587\n","iteration no: 359, loss: 0.06297040730714798\n","iteration no: 360, loss: 0.06081593781709671\n","iteration no: 361, loss: 0.058701805770397186\n","iteration no: 362, loss: 0.05666792020201683\n","iteration no: 363, loss: 0.05472321808338165\n","iteration no: 364, loss: 0.05284872278571129\n","iteration no: 365, loss: 0.05101652070879936\n","iteration no: 366, loss: 0.04925575852394104\n","iteration no: 367, loss: 0.047556232661008835\n","iteration no: 368, loss: 0.04592030495405197\n","iteration no: 369, loss: 0.04434319585561752\n","iteration no: 370, loss: 0.04282025247812271\n","iteration no: 371, loss: 0.041354186832904816\n","iteration no: 372, loss: 0.03993157297372818\n","iteration no: 373, loss: 0.038570426404476166\n","iteration no: 374, loss: 0.037238653749227524\n","iteration no: 375, loss: 0.035969607532024384\n","iteration no: 376, loss: 0.03472598269581795\n","iteration no: 377, loss: 0.03354788199067116\n","iteration no: 378, loss: 0.03238851577043533\n","iteration no: 379, loss: 0.03128249943256378\n","iteration no: 380, loss: 0.030208464711904526\n","iteration no: 381, loss: 0.02917819656431675\n","iteration no: 382, loss: 0.028187086805701256\n","iteration no: 383, loss: 0.027235472574830055\n","iteration no: 384, loss: 0.02630421705543995\n","iteration no: 385, loss: 0.025404904037714005\n","iteration no: 386, loss: 0.024540215730667114\n","iteration no: 387, loss: 0.023714307695627213\n","iteration no: 388, loss: 0.0228964202105999\n","iteration no: 389, loss: 0.02212030254304409\n","iteration no: 390, loss: 0.021370582282543182\n","iteration no: 391, loss: 0.020639078691601753\n","iteration no: 392, loss: 0.019939422607421875\n","iteration no: 393, loss: 0.01925356313586235\n","iteration no: 394, loss: 0.018611283972859383\n","iteration no: 395, loss: 0.017982859164476395\n","iteration no: 396, loss: 0.01736713945865631\n","iteration no: 397, loss: 0.016780823469161987\n","iteration no: 398, loss: 0.016215156763792038\n","iteration no: 399, loss: 0.015669330954551697\n","iteration no: 400, loss: 0.015135384164750576\n","iteration no: 401, loss: 0.014636227861046791\n","iteration no: 402, loss: 0.014146573841571808\n","iteration no: 403, loss: 0.013670753687620163\n","iteration no: 404, loss: 0.013208307325839996\n","iteration no: 405, loss: 0.012771341018378735\n","iteration no: 406, loss: 0.012346770614385605\n","iteration no: 407, loss: 0.011926648207008839\n","iteration no: 408, loss: 0.011534074321389198\n","iteration no: 409, loss: 0.011148639023303986\n","iteration no: 410, loss: 0.010783123783767223\n","iteration no: 411, loss: 0.010424564592540264\n","iteration no: 412, loss: 0.01008177362382412\n","iteration no: 413, loss: 0.009748747572302818\n","iteration no: 414, loss: 0.009422119706869125\n","iteration no: 415, loss: 0.009114746004343033\n","iteration no: 416, loss: 0.008811955340206623\n","iteration no: 417, loss: 0.00851917639374733\n","iteration no: 418, loss: 0.008251429535448551\n","iteration no: 419, loss: 0.007972320541739464\n","iteration no: 420, loss: 0.007711463142186403\n","iteration no: 421, loss: 0.007458403240889311\n","iteration no: 422, loss: 0.007222057785838842\n","iteration no: 423, loss: 0.006982808001339436\n","iteration no: 424, loss: 0.006757478229701519\n","iteration no: 425, loss: 0.006537355948239565\n","iteration no: 426, loss: 0.006324940361082554\n","iteration no: 427, loss: 0.006121783051639795\n","iteration no: 428, loss: 0.00592462345957756\n","iteration no: 429, loss: 0.0057366350665688515\n","iteration no: 430, loss: 0.005554813425987959\n","iteration no: 431, loss: 0.005374835338443518\n","iteration no: 432, loss: 0.005204248242080212\n","iteration no: 433, loss: 0.005041779018938541\n","iteration no: 434, loss: 0.004882505163550377\n","iteration no: 435, loss: 0.00473336037248373\n","iteration no: 436, loss: 0.004583577159792185\n","iteration no: 437, loss: 0.004438292700797319\n","iteration no: 438, loss: 0.004298487212508917\n","iteration no: 439, loss: 0.004163087345659733\n","iteration no: 440, loss: 0.004038142040371895\n","iteration no: 441, loss: 0.0039159879088401794\n","iteration no: 442, loss: 0.0037940109614282846\n","iteration no: 443, loss: 0.003678266890347004\n","iteration no: 444, loss: 0.0035676448605954647\n","iteration no: 445, loss: 0.0034592419397085905\n","iteration no: 446, loss: 0.003352419938892126\n","iteration no: 447, loss: 0.003255722811445594\n","iteration no: 448, loss: 0.0031568962149322033\n","iteration no: 449, loss: 0.0030657188035547733\n","iteration no: 450, loss: 0.002973452676087618\n","iteration no: 451, loss: 0.002884642221033573\n","iteration no: 452, loss: 0.002796873450279236\n","iteration no: 453, loss: 0.0027159727178514004\n","iteration no: 454, loss: 0.0026372657157480717\n","iteration no: 455, loss: 0.002558669075369835\n","iteration no: 456, loss: 0.002483635675162077\n","iteration no: 457, loss: 0.0024138945154845715\n","iteration no: 458, loss: 0.0023428958375006914\n","iteration no: 459, loss: 0.002273805672302842\n","iteration no: 460, loss: 0.002210661768913269\n","iteration no: 461, loss: 0.0021479884162545204\n","iteration no: 462, loss: 0.0020852009765803814\n","iteration no: 463, loss: 0.0020264226477593184\n","iteration no: 464, loss: 0.001969733042642474\n","iteration no: 465, loss: 0.0019141675438731909\n","iteration no: 466, loss: 0.0018623899668455124\n","iteration no: 467, loss: 0.0018088773358613253\n","iteration no: 468, loss: 0.0017576399259269238\n","iteration no: 469, loss: 0.001712131779640913\n","iteration no: 470, loss: 0.0016633349005132914\n","iteration no: 471, loss: 0.0016195852076634765\n","iteration no: 472, loss: 0.0015738909132778645\n","iteration no: 473, loss: 0.0015290080336853862\n","iteration no: 474, loss: 0.0014905050629749894\n","iteration no: 475, loss: 0.0014510878827422857\n","iteration no: 476, loss: 0.0014108993345871568\n","iteration no: 477, loss: 0.001374658546410501\n","iteration no: 478, loss: 0.0013381341705098748\n","iteration no: 479, loss: 0.0013028678949922323\n","iteration no: 480, loss: 0.0012713312171399593\n","iteration no: 481, loss: 0.001237044227309525\n","iteration no: 482, loss: 0.0012046375777572393\n","iteration no: 483, loss: 0.0011744463117793202\n","iteration no: 484, loss: 0.0011445884592831135\n","iteration no: 485, loss: 0.001116287661716342\n","iteration no: 486, loss: 0.0010879328474402428\n","iteration no: 487, loss: 0.0010598411317914724\n","iteration no: 488, loss: 0.0010342494351789355\n","iteration no: 489, loss: 0.0010093323653563857\n","iteration no: 490, loss: 0.0009846232132986188\n","iteration no: 491, loss: 0.0009589562541805208\n","iteration no: 492, loss: 0.0009367237216793001\n","iteration no: 493, loss: 0.0009138942114077508\n","iteration no: 494, loss: 0.0008912336779758334\n","iteration no: 495, loss: 0.0008689513779245317\n","iteration no: 496, loss: 0.0008490331238135695\n","iteration no: 497, loss: 0.0008286035736091435\n","iteration no: 498, loss: 0.0008089569164440036\n","iteration no: 499, loss: 0.0007892189314588904\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f585fb2b400>]"]},"metadata":{"tags":[]},"execution_count":4},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDChYAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ECZ1PK49hXvx","colab_type":"code","outputId":"621b7d7b-5912-478b-fb8c-ed7d447bd11c","executionInfo":{"status":"ok","timestamp":1586455862421,"user_tz":-330,"elapsed":10343,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#making self autograd fxns.\n","\n","class MyRelu(torch.autograd.Function):\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","\n","        ctx.save_for_backward(input)\n","        return input.clamp(min=0)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        input, =ctx.saved_tensors\n","        grad_input=grad_output.clone()\n","        grad_input[input<0]=0\n","        return grad_input\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x = torch.randn(N, D_in, device=device, dtype=dtype)\n","y = torch.randn(N, D_out, device=device, dtype=dtype)\n","\n","w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n","w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n","    relu = MyRelu.apply\n","\n","    y_pred = relu(x.mm(w1)).mm(w2)\n","\n","    # Compute and print loss\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # Use autograd to compute the backward pass.\n","    loss.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        w1 -= learning_rate * w1.grad\n","        w2 -= learning_rate * w2.grad\n","\n","        # Manually zero the gradients after updating weights\n","        w1.grad.zero_()\n","        w2.grad.zero_()\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99 874.4266357421875\n","199 5.93480920791626\n","299 0.06529156863689423\n","399 0.0011585685424506664\n","499 0.00011111088679172099\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uk1FD86o1kUD","colab_type":"code","outputId":"80a5c5d8-ba28-4f2a-d565-ce16b5f701b3","executionInfo":{"status":"ok","timestamp":1586455863058,"user_tz":-330,"elapsed":10274,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["#Using nn module\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x=torch.randn(N, D_in)\n","y=torch.randn(N, D_out)\n","\n","model=torch.nn.Sequential(torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out))\n","                                            \n","\n","loss_fn = torch.nn.MSELoss(reduce='sum')\n","\n","lr=1e-6\n","\n","for t in range(500):\n","    y_pred=model(x)\n","    \n","    cost=loss_fn(y_pred, y)\n","\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    model.zero_grad()\n","\n","    cost.backward()\n","\n","    with torch.no_grad():\n","        \n","        for para in model.parameters():\n","            para-=lr*para.grad"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99 0.00011111088679172099\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["199 0.00011111088679172099\n","299 0.00011111088679172099\n","399 0.00011111088679172099\n","499 0.00011111088679172099\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uQVagvLA5b0g","colab_type":"code","outputId":"05ae66a6-b5d9-4d3e-ce9a-aa5bc32545ea","executionInfo":{"status":"ok","timestamp":1586459400344,"user_tz":-330,"elapsed":2011,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#using optimiser\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(H, D_out),\n",")\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","#defining optimiser\n","\n","lr=1e-6\n","\n","optimiser=optim.Adam(model.parameters(), lr=lr)\n","\n","for t in range(500):\n","    y_pred=model(x)\n","\n","    cost=loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, cost.item())\n","\n","    optimiser.zero_grad()\n","    cost.backward()\n","\n","    optimiser.step()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99 602.9542236328125\n","199 587.3330688476562\n","299 572.191650390625\n","399 557.6245727539062\n","499 543.5125732421875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7iyTMKBaLpBy","colab_type":"code","outputId":"3a439dd7-4635-449b-fb26-269bd8aa8456","executionInfo":{"status":"ok","timestamp":1586459455278,"user_tz":-330,"elapsed":1870,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#defining our own model\n","\n","class self_net(torch.nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","\n","        super(self_net, self).__init__()\n","\n","        self.linear1=torch.nn.Linear(D_in, H)\n","        self.linear2=torch.nn.Linear(H, D_out)\n","\n","    def forward(self, x):\n","        h_relu=self.linear1(x).clamp(min=0)\n","        y_pred=self.linear2(h_relu)\n","\n","        return y_pred\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","model=self_net(D_in, H, D_out)\n","loss=torch.nn.MSELoss(reduction='sum')\n","optimiser=optim.Adam(model.parameters(), lr=1e-4)\n","\n","for i in range(500):\n","\n","    y_pred=model(x)\n","    cost=loss(y_pred, y)\n","    if i%100 == 99 :\n","        print(i, cost.item())\n","    optimiser.zero_grad()\n","    cost.backward()\n","    optimiser.step()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99 56.59048080444336\n","199 0.858749270439148\n","299 0.0076751708984375\n","399 4.652299321605824e-05\n","499 9.377489362805136e-08\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ApfahIikP01A","colab_type":"code","outputId":"3ffe7246-fa84-4952-edaa-a54146be67b8","executionInfo":{"status":"ok","timestamp":1586461063991,"user_tz":-330,"elapsed":2005,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#control flow + weight sharing \n","\n","class DynamicNet(torch.nn.Module):\n","\n","    def __init__(self, D_in, H, D_out):\n","\n","        super(DynamicNet, self).__init__()\n","\n","        self.input_linear=torch.nn.Linear(D_in, H)\n","        self.middle_linear=torch.nn.Linear(H, H)\n","        self.output_linear=torch.nn.Linear(H, D_out)\n","\n","    def forward(self, x):\n","\n","        h_relu=self.input_linear(x).clamp(min=0)\n","        \n","        for i in range(random.randint(0, 3)):\n","            h_relu=self.middle_linear(h_relu).clamp(min=0)\n","\n","        y_pred=self.output_linear(h_relu)\n","\n","        return y_pred\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x=torch.randn(N, D_in)\n","y=torch.randn(N, D_out)\n","\n","model=DynamicNet(D_in, H, D_out)\n","\n","loss=torch.nn.MSELoss(reduction='sum')\n","optimiser=optim.SGD(model.parameters(), lr=1e-4, momentum=0.8)\n","\n","for t  in range(500):\n","\n","    y_pred=model(x)\n","\n","    cost=loss(y_pred, y)\n","\n","    if t%100 == 99:\n","        print(t, cost)\n","\n","    optimiser.zero_grad()\n","    cost.backward()\n","    optimiser.step()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99 tensor(24.3545, grad_fn=<MseLossBackward>)\n","199 tensor(19.3616, grad_fn=<MseLossBackward>)\n","299 tensor(3.3757, grad_fn=<MseLossBackward>)\n","399 tensor(0.3158, grad_fn=<MseLossBackward>)\n","499 tensor(5.1351, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3y9zWx-lcPFR","colab_type":"code","outputId":"435d22da-3376-4ebc-e90a-8e596f9697b0","executionInfo":{"status":"error","timestamp":1586461863063,"user_tz":-330,"elapsed":1688,"user":{"displayName":"CHIRAG GARG","photoUrl":"","userId":"09378163611563108853"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","# First we set up the computational graph:\n","\n","# N is batch size; D_in is input dimension;\n","# H is hidden dimension; D_out is output dimension.\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# Create placeholders for the input and target data; these will be filled\n","# with real data when we execute the graph.\n","x = tf.placeholder(tf.float32, shape=(None, D_in))\n","y = tf.placeholder(tf.float32, shape=(None, D_out))\n","\n","# Create Variables for the weights and initialize them with random data.\n","# A TensorFlow Variable persists its value across executions of the graph.\n","w1 = tf.Variable(tf.random_normal((D_in, H)))\n","w2 = tf.Variable(tf.random_normal((H, D_out)))\n","\n","# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n","# Note that this code does not actually perform any numeric operations; it\n","# merely sets up the computational graph that we will later execute.\n","h = tf.matmul(x, w1)\n","h_relu = tf.maximum(h, tf.zeros(1))\n","y_pred = tf.matmul(h_relu, w2)\n","\n","# Compute loss using operations on TensorFlow Tensors\n","loss = tf.reduce_sum((y - y_pred) ** 2.0)\n","\n","# Compute gradient of the loss with respect to w1 and w2.\n","grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n","\n","# Update the weights using gradient descent. To actually update the weights\n","# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n","# in TensorFlow the the act of updating the value of the weights is part of\n","# the computational graph; in PyTorch this happens outside the computational\n","# graph.\n","learning_rate = 1e-6\n","new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n","new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n","\n","# Now we have built our computational graph, so we enter a TensorFlow session to\n","# actually execute the graph.\n","with tf.Session() as sess:\n","    # Run the graph once to initialize the Variables w1 and w2.\n","    sess.run(tf.global_variables_initializer())\n","\n","    # Create numpy arrays holding the actual data for the inputs x and targets\n","    # y\n","    x_value = np.random.randn(N, D_in)\n","    y_value = np.random.randn(N, D_out)\n","    for t in range(500):\n","        # Execute the graph many times. Each time it executes we want to bind\n","        # x_value to x and y_value to y, specified with the feed_dict argument.\n","        # Each time we execute the graph we want to compute the values for loss,\n","        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n","        # arrays.\n","        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n","                                    feed_dict={x: x_value, y: y_value})\n","        if t % 100 == 99:\n","            print(t, loss_value)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-efe01fc1446b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create placeholders for the input and target data; these will be filled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# with real data when we execute the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"]}]},{"cell_type":"code","metadata":{"id":"vNs3irTzfclG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}